{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ragas\n",
    "- collects analytics\n",
    "- can be costly https://openai.com/api/pricing/\n",
    "\n",
    "Observations\n",
    "- sometimes generates duplicate examples\n",
    "\n",
    "Src\n",
    "- https://github.com/explodinggradients/ragas\n",
    "- https://docs.ragas.io/en/stable/getstarted/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ragas langchain langchain_community unstructured python-magic-bin==0.4.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d05328b3a24e78ac2ab2652e6cdcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_correctness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was the first super bowl?</td>\n",
       "      <td>[The First AFL–NFL World Championship Game was...</td>\n",
       "      <td>The first superbowl was held on Jan 15, 1967</td>\n",
       "      <td>The first superbowl was held on January 15, 1967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who won the most super bowls?</td>\n",
       "      <td>[The Green Bay Packers...Green Bay, Wisconsin....</td>\n",
       "      <td>The most super bowls have been won by The New ...</td>\n",
       "      <td>The New England Patriots have won the Super Bo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.981086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         question  \\\n",
       "0  When was the first super bowl?   \n",
       "1   Who won the most super bowls?   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [The First AFL–NFL World Championship Game was...   \n",
       "1  [The Green Bay Packers...Green Bay, Wisconsin....   \n",
       "\n",
       "                                              answer  \\\n",
       "0       The first superbowl was held on Jan 15, 1967   \n",
       "1  The most super bowls have been won by The New ...   \n",
       "\n",
       "                                        ground_truth  faithfulness  \\\n",
       "0   The first superbowl was held on January 15, 1967           1.0   \n",
       "1  The New England Patriots have won the Super Bo...           0.0   \n",
       "\n",
       "   answer_correctness  \n",
       "0            0.999093  \n",
       "1            0.981086  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quickstart\n",
    "from datasets import Dataset \n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_correctness\n",
    "\n",
    "data_samples = {\n",
    "    'question': ['When was the first super bowl?', 'Who won the most super bowls?'],\n",
    "    'answer': ['The first superbowl was held on Jan 15, 1967', 'The most super bowls have been won by The New England Patriots'],\n",
    "    'contexts' : [['The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles,'], \n",
    "    ['The Green Bay Packers...Green Bay, Wisconsin.','The Packers compete...Football Conference']],\n",
    "    'ground_truth': ['The first superbowl was held on January 15, 1967', 'The New England Patriots have won the Super Bowl a record six times']\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "score = evaluate(dataset,metrics=[faithfulness,answer_correctness])\n",
    "score.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PDF parse\n",
    "# Fix nltk punkt error: OSError: No such file or directory: '/Users/az/nltk_data/tokenizers/punkt/PY3_tab'\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "!mv /Users/az/nltk_data/tokenizers/punkt/PY3/ /Users/az/nltk_data/tokenizers/punkt/PY3_tab/ # possibly wrong but works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/ignored/1_paper/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(path)\n",
    "documents = loader.load()\n",
    "for document in documents:\n",
    "    document.metadata['filename'] = document.metadata['source']\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eacbe702a474c04aa58e6d5a4bab8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d988da0ea0a414d81d6a31def5e2b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4o-2024-08-06\")\n",
    "# critic_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# generate testset\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=5, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_path = \"data/ragas_attention_testset.csv\"\n",
    "testset.to_pandas().to_csv(testset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ba2398ec614d28873b278a7b646466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'question', 'contexts', 'ground_truth', 'evolution_type', 'metadata', 'episode_done'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "testset = load_dataset(\"csv\", data_files=testset_path)\n",
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix reading list from csv bc ValidationError at evaluate\n",
    "import json\n",
    "def str_to_list(row: dict) -> dict:\n",
    "    row[\"contexts\"] = json.loads(row[\"contexts\"].replace(\"'\", \"\\\"\")) # replace bc json only allows \"\", not ''\n",
    "    return row\n",
    "testset = testset.map(str_to_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "ValidationError: 1 validation error for SingleTurnSample\n",
    "retrieved_contexts\n",
    "  value is not a valid list (type=type_error.list)\n",
    "```\n",
    "\n",
    "Field `retrieved_contexts` was read as string:\n",
    "\n",
    "```'contexts': \"[' 28.4...']\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2184cb72ccb24dde8817770341a7b6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.9000, 'faithfulness': nan, 'answer_relevancy': 0.1691, 'context_recall': 0.8500}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://docs.ragas.io/en/stable/concepts/metrics/index.html#ragas-metrics\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    testset[\"train\"],\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_path = \"data/ragas_attention_evaluation.csv\"\n",
    "result.to_pandas().to_csv(evaluation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
